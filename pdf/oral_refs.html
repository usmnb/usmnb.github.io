<h1> Failure Modes in Unsupervised Image-to-Image Translation </h1>
<h2> Main references: </h2>

<ul>
<li> “Generalization and Equilibrium in Generative Adversarial Nets” by Arora et al., PMLR 2017. </li>
<li> “Training Generative Adversarial Networks with Limited Data” by Karras et al., NeurIPS 2020. </li>
<li> “Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs” by Galanti et al., JMLR 2021. // 2017-2021 </li>
</ul>

<h2> Other references: </h2>

<li> “Simple Strategies for Large Zero-Sum Games with Applications to Complexity Theory” by Lipton & Young, STOC’94</li>
<li> “Foundations of Machine Learning” Mohri, Rostamizadeh, Talwalkar, 2nd Edition, 2018 </li>
<li> “Towards Principled Methods for Training Generative Adversarial Networks”, Arjovsky & Bottou, ICLR’17  </li>

<li> “Stabilizing Training of Generative Adversarial Networks through Regularization”, Roth et al, NeurIPS’17</li>

<li> “Which Training Methods for GANs do actually Converge?”, Mescheder et al., ICML’18</li>

<li> “Kernel of CycleGAN as a Principle homogeneous space”, Moriakov et al., ICLR’20</li>

<li> “Guiding the One-to-One Mapping in CycleGAN via Optimal Transport’, Lu et al., AAAI’19
</li>

